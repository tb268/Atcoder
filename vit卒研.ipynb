{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tb268/Atcoder/blob/main/vit%E5%8D%92%E7%A0%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "# フォルダを作成する\n",
        "os.makedirs('./data')\n"
      ],
      "metadata": {
        "id": "a9GWBE-_uFs7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "reu5itblmPFJ"
      },
      "outputs": [],
      "source": [
        "#フォルダ削除用\n",
        "import shutil\n",
        "shutil.rmtree('/content/data')\n",
        "\n",
        "# フォルダを作成する\n",
        "os.makedirs('./data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FojPJM00al3j",
        "outputId": "a4d61452-3b15-41ce-ad25-899446f6f6c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting reformer-pytorch\n",
            "  Downloading reformer_pytorch-1.4.4-py3-none-any.whl (16 kB)\n",
            "Collecting axial-positional-embedding>=0.1.0 (from reformer-pytorch)\n",
            "  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting einops (from reformer-pytorch)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting local-attention (from reformer-pytorch)\n",
            "  Downloading local_attention-1.9.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting product-key-memory (from reformer-pytorch)\n",
            "  Downloading product_key_memory-0.2.10-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from reformer-pytorch) (2.2.1+cu121)\n",
            "Collecting colt5-attention>=0.10.14 (from product-key-memory->reformer-pytorch)\n",
            "  Downloading CoLT5_attention-0.10.20-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->reformer-pytorch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->reformer-pytorch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->reformer-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->reformer-pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->reformer-pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->reformer-pytorch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->reformer-pytorch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->reformer-pytorch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->reformer-pytorch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->reformer-pytorch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->reformer-pytorch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->reformer-pytorch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->reformer-pytorch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->reformer-pytorch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->reformer-pytorch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->reformer-pytorch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->reformer-pytorch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->reformer-pytorch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->reformer-pytorch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colt5-attention>=0.10.14->product-key-memory->reformer-pytorch) (24.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->reformer-pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->reformer-pytorch) (1.3.0)\n",
            "Building wheels for collected packages: axial-positional-embedding\n",
            "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2882 sha256=1cc154feda8face50a36a2cc7e52077add029bd7cc3d9d4f9f804e4a4bcce3ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/cb/39/7ce7ff2d2fd37cfe1fe7b3a3c43cf410632b2ad3b3f3986d73\n",
            "Successfully built axial-positional-embedding\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, einops, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, local-attention, axial-positional-embedding, colt5-attention, product-key-memory, reformer-pytorch\n",
            "Successfully installed axial-positional-embedding-0.2.1 colt5-attention-0.10.20 einops-0.8.0 local-attention-1.9.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 product-key-memory-0.2.10 reformer-pytorch-1.4.4\n"
          ]
        }
      ],
      "source": [
        "!pip install reformer-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZicjhbqkU_x",
        "outputId": "8c9c563d-70e0-4d9f-86a8-ee7cc9d65ca1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed May 15 07:30:42 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EJP0zQMtuL3I"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RTOKWC-bvEaU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e11tusa0yCEA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reformer版 3D Pose Estimation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from reformer_pytorch import Reformer, Autopadder\n",
        "\n",
        "class PoseReformer3D(nn.Module):\n",
        "    def __init__(self, patch_size=32, num_keypoints=17, dim=512, depth=6, heads=8, bucket_size=16, n_hashes=4):\n",
        "        super(PoseReformer3D, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_keypoints = num_keypoints\n",
        "        self.dim = dim\n",
        "        # パッチ埋め込み (Patch Embedding)\n",
        "        self.patch_to_embedding = nn.Linear(3 * patch_size * patch_size, dim)\n",
        "        # クラストークン (Class Token)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        # Reformer モデルの定義\n",
        "        self.reformer = Reformer(\n",
        "            dim=dim,\n",
        "            depth=depth,\n",
        "            heads=heads,\n",
        "            bucket_size=bucket_size,\n",
        "            n_hashes=n_hashes\n",
        "        )\n",
        "        self.reformer = Autopadder(self.reformer)\n",
        "        self.to_cls_token = nn.Identity()\n",
        "        # 最終的なMLPヘッド\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_keypoints * 3)  # 3次元座標 (x, y, z) なので num_keypoints * 3\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        B, C, H, W = img.shape\n",
        "        p = self.patch_size\n",
        "        # 画像ピクセルをパッチサイズに分割\n",
        "        x = img.unfold(2, p, p).unfold(3, p, p)  # (B, C, H/p, W/p, p, p)\n",
        "        x = x.permute(0, 2, 3, 1, 4, 5)  # (B, H/p, W/p, C, p, p)\n",
        "        x = x.reshape(x.shape[0], -1, C * p * p)  # (B, num_patches, patch_dim)\n",
        "        num_patches = x.shape[1]\n",
        "        # パッチ埋め込み (Patch Embedding)\n",
        "        x = self.patch_to_embedding(x)  # (B, num_patches, dim)\n",
        "\n",
        "        # クラストークン (Class Token)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, dim)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches + 1, dim)\n",
        "\n",
        "        # 位置埋め込み (Position Embedding)\n",
        "        pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, self.dim))\n",
        "        x += pos_embedding[:, :(x.size(1))]\n",
        "\n",
        "        # Reformer での処理\n",
        "        x = self.reformer(x)  # (B, num_patches + 1, dim)\n",
        "        x = self.to_cls_token(x[:, 0])  # クラストークンの取得\n",
        "        # 最終的なMLPヘッドでキーポイントを予測\n",
        "        keypoints = self.mlp_head(x)  # (B, num_keypoints * 3)\n",
        "        keypoints = keypoints.view(x.size(0), -1, 3)  # (B, num_keypoints, 3)\n",
        "        return keypoints\n",
        "\n",
        "def generate_dummy_data(num_samples, num_keypoints, img_height, img_width):\n",
        "    images = torch.randn(num_samples, 3, img_height, img_width)  # ダミー画像データ\n",
        "    keypoints = torch.randn(num_samples, num_keypoints, 3) * 1000  # ダミー3Dキーポイント（mm単位）\n",
        "    return images, keypoints\n",
        "\n",
        "def compute_mpjpe(predicted, true):\n",
        "    return torch.mean(torch.norm(predicted - true, dim=-1))  # 各キーポイント間の距離を計算し、平均を取る\n",
        "\n",
        "# ダミーデータ生成\n",
        "num_samples = 100\n",
        "num_keypoints = 17\n",
        "img_height, img_width = 320, 240  # 任意の画像サイズ\n",
        "images, true_keypoints = generate_dummy_data(num_samples, num_keypoints, img_height, img_width)\n",
        "\n",
        "# モデルのインスタンス化\n",
        "model = PoseReformer3D()\n",
        "model.eval()  # 評価モードに切り替え\n",
        "\n",
        "# 推論の実行\n",
        "with torch.no_grad():\n",
        "    predicted_keypoints = model(images)\n",
        "\n",
        "# MPJPEの計算\n",
        "mpjpe = compute_mpjpe(predicted_keypoints, true_keypoints)\n",
        "print(f'Average MPJPE: {mpjpe.item():.2f} mm')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhbKKfpCBSYm",
        "outputId": "4b41f498-798c-4660-8fd7-f77fb7dfba70"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MPJPE: 1625.53 mm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transformer版 3D Pose Estimation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.nn import Transformer\n",
        "class PoseTransformer3D(nn.Module):\n",
        "    def __init__(self, image_size=256, patch_size=32, num_keypoints=17, dim=512, depth=6, heads=8):\n",
        "        super(PoseTransformer3D, self).__init__()\n",
        "        assert image_size % patch_size == 0, 'Image size must be divisible by the patch size'\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = 3 * patch_size * patch_size\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        # 位置埋め込み (Position Embedding)\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))  # +1 for class token\n",
        "        # パッチ埋め込み (Patch Embedding)\n",
        "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
        "        # クラストークン (Class Token)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        # Transformer モデルの定義\n",
        "        self.transformer = Transformer(\n",
        "            d_model=dim,\n",
        "            nhead=heads,\n",
        "            num_encoder_layers=depth,\n",
        "            num_decoder_layers=depth,\n",
        "            dim_feedforward=dim * 4,\n",
        "            dropout=0.1\n",
        "        )\n",
        "        self.to_cls_token = nn.Identity()\n",
        "        # 最終的なMLPヘッド\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_keypoints * 3)  # 3次元座標 (x, y, z) なので num_keypoints * 3\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        p = self.patch_size\n",
        "        # 画像ピクセルを32x32のパッチに分割\n",
        "        x = img.unfold(2, p, p).unfold(3, p, p)  # (B, C, H/p, W/p, p, p)\n",
        "        x = x.permute(0, 2, 3, 1, 4, 5)  # (B, H/p, W/p, C, p, p)\n",
        "        x = x.reshape(x.shape[0], -1, 3 * p * p)  # (B, num_patches, patch_dim)\n",
        "        # パッチ埋め込み (Patch Embedding)\n",
        "        x = self.patch_to_embedding(x)  # (B, num_patches, dim)\n",
        "\n",
        "        # クラストークン (Class Token)\n",
        "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)  # (B, 1, dim)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches + 1, dim)\n",
        "\n",
        "        # 位置埋め込み (Position Embedding)\n",
        "        x += self.pos_embedding[:, :(x.size(1))]\n",
        "\n",
        "        # Transformer での処理\n",
        "        x = self.transformer(x, x)  # (B, num_patches + 1, dim)\n",
        "        x = self.to_cls_token(x[:, 0])  # クラストークンの取得\n",
        "        # 最終的なMLPヘッドでキーポイントを予測\n",
        "        keypoints = self.mlp_head(x)  # (B, num_keypoints * 3)\n",
        "        keypoints = keypoints.view(x.size(0), -1, 3)  # (B, num_keypoints, 3)\n",
        "        return keypoints\n",
        "def generate_dummy_data(num_samples, num_keypoints):\n",
        "    images = torch.randn(num_samples, 3, 256, 256)  # ダミー画像データ\n",
        "    keypoints = torch.randn(num_samples, num_keypoints, 3) * 1000  # ダミー3Dキーポイント（mm単位）\n",
        "    return images, keypoints\n",
        "\n",
        "num_samples = 100\n",
        "num_keypoints = 17\n",
        "images, true_keypoints = generate_dummy_data(num_samples, num_keypoints)\n",
        "def compute_mpjpe(predicted, true):\n",
        "    return torch.mean(torch.norm(predicted - true, dim=-1))  # 各キーポイント間の距離を計算し、平均を取る\n",
        "# モデルのインスタンス化\n",
        "model = PoseTransformer3D()\n",
        "model.eval()  # 評価モードに切り替え\n",
        "\n",
        "# 推論の実行\n",
        "with torch.no_grad():\n",
        "    predicted_keypoints = model(images)\n",
        "\n",
        "# MPJPEの計算\n",
        "mpjpe = compute_mpjpe(predicted_keypoints, true_keypoints)\n",
        "print(f'Average MPJPE: {mpjpe.item():.2f} mm')\n"
      ],
      "metadata": {
        "id": "6jwX6vvWCIS5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f52b48bd-f9ea-4756-e38b-95056adff142"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MPJPE: 1576.79 mm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZqWMvYrMFf3l"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMoOYMIaauGJC0LkvSaOLCA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}