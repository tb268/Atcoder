{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tb268/Atcoder/blob/main/vit%E5%8D%92%E7%A0%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install reformer-pytorch"
      ],
      "metadata": {
        "id": "9ZxHqwJ4dx61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# フォルダを作成する\n",
        "os.makedirs('./data')"
      ],
      "metadata": {
        "id": "a9GWBE-_uFs7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "reu5itblmPFJ"
      },
      "outputs": [],
      "source": [
        "#フォルダ削除用\n",
        "import shutil\n",
        "shutil.rmtree('/content/data')\n",
        "\n",
        "# フォルダを作成する\n",
        "os.makedirs('./data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZicjhbqkU_x",
        "outputId": "8c9c563d-70e0-4d9f-86a8-ee7cc9d65ca1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed May 15 07:30:42 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EJP0zQMtuL3I"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RTOKWC-bvEaU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e11tusa0yCEA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reformer版 3D Pose Estimation\n",
        "# ダミーデータでのコード\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from reformer_pytorch import Reformer, Autopadder\n",
        "\n",
        "# モデルの定義\n",
        "class PoseReformer3D(nn.Module):\n",
        "    def __init__(self, patch_size=32, num_keypoints=17, dim=512, depth=6, heads=8, bucket_size=16, n_hashes=4):\n",
        "        super(PoseReformer3D, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_keypoints = num_keypoints\n",
        "        self.dim = dim\n",
        "        # パッチ埋め込み (Patch Embedding)\n",
        "        self.patch_to_embedding = nn.Linear(3 * patch_size * patch_size, dim)\n",
        "        # クラストークン (Class Token)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        # Reformer モデルの定義\n",
        "        self.reformer = Reformer(\n",
        "            dim=dim,\n",
        "            depth=depth,\n",
        "            heads=heads,\n",
        "            bucket_size=bucket_size,\n",
        "            n_hashes=n_hashes\n",
        "        )\n",
        "        self.reformer = Autopadder(self.reformer)\n",
        "        self.to_cls_token = nn.Identity()\n",
        "        # 最終的なMLPヘッド\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_keypoints * 3)  # 3次元座標 (x, y, z) なので num_keypoints * 3\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        B, C, H, W = img.shape\n",
        "        p = self.patch_size\n",
        "        # 画像ピクセルをパッチサイズに分割\n",
        "        x = img.unfold(2, p, p).unfold(3, p, p)  # (B, C, H/p, W/p, p, p)\n",
        "        x = x.permute(0, 2, 3, 1, 4, 5)  # (B, H/p, W/p, C, p, p)\n",
        "        x = x.reshape(x.shape[0], -1, C * p * p)  # (B, num_patches, patch_dim)\n",
        "        num_patches = x.shape[1]\n",
        "        # パッチ埋め込み (Patch Embedding)\n",
        "        x = self.patch_to_embedding(x)  # (B, num_patches, dim)\n",
        "\n",
        "        # クラストークン (Class Token)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, dim)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches + 1, dim)\n",
        "\n",
        "        # 位置埋め込み (Position Embedding)\n",
        "        pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, self.dim))\n",
        "        x += pos_embedding[:, :(x.size(1))]\n",
        "\n",
        "        # Reformer での処理\n",
        "        x = self.reformer(x)  # (B, num_patches + 1, dim)\n",
        "        x = self.to_cls_token(x[:, 0])  # クラストークンの取得\n",
        "        # 最終的なMLPヘッドでキーポイントを予測\n",
        "        keypoints = self.mlp_head(x)  # (B, num_keypoints * 3)\n",
        "        keypoints = keypoints.view(x.size(0), -1, 3)  # (B, num_keypoints, 3)\n",
        "        return keypoints\n",
        "\n",
        "# ダミーデータ生成関数\n",
        "def generate_dummy_data(num_samples, num_keypoints, img_height, img_width):\n",
        "    images = torch.randn(num_samples, 3, img_height, img_width)  # ダミー画像データ\n",
        "    keypoints = torch.randn(num_samples, num_keypoints, 3) * 1000  # ダミー3Dキーポイント（mm単位）\n",
        "    return images, keypoints\n",
        "\n",
        "# MPJPE計算関数\n",
        "def compute_mpjpe(predicted, true):\n",
        "    return torch.mean(torch.norm(predicted - true, dim=-1))  # 各キーポイント間の距離を計算し、平均を取る\n",
        "\n",
        "# トレーニング関数\n",
        "def train_model(model, images, true_keypoints, epochs=10, learning_rate=0.001):\n",
        "    model.train()  # トレーニングモードに切り替え\n",
        "    criterion = nn.MSELoss()  # 損失関数\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # オプティマイザ\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()  # 勾配の初期化\n",
        "        predicted_keypoints = model(images)  # 推論\n",
        "        loss = criterion(predicted_keypoints, true_keypoints)  # 損失の計算\n",
        "        loss.backward()  # 勾配の計算\n",
        "        optimizer.step()  # パラメータの更新\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}')\n",
        "\n",
        "# ダミーデータ生成\n",
        "num_samples = 100\n",
        "num_keypoints = 17\n",
        "img_height, img_width = 320, 240  # 任意の画像サイズ\n",
        "images, true_keypoints = generate_dummy_data(num_samples, num_keypoints, img_height, img_width)\n",
        "\n",
        "# モデルのインスタンス化\n",
        "model = PoseReformer3D()\n",
        "\n",
        "# モデルのトレーニング\n",
        "train_model(model, images, true_keypoints, epochs=20, learning_rate=0.001)\n",
        "\n",
        "# 評価モードに切り替え\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predicted_keypoints = model(images)\n",
        "\n",
        "# MPJPEの計算\n",
        "mpjpe = compute_mpjpe(predicted_keypoints, true_keypoints)\n",
        "print(f'Average MPJPE: {mpjpe.item():.2f} mm')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhbKKfpCBSYm",
        "outputId": "0a4eec5b-ecc0-4b48-ed5b-ceba8f905b6e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 991823.6875\n",
            "Epoch 2/20, Loss: 991529.7500\n",
            "Epoch 3/20, Loss: 991393.5000\n",
            "Epoch 4/20, Loss: 991319.9375\n",
            "Epoch 5/20, Loss: 991254.1875\n",
            "Epoch 6/20, Loss: 991175.0625\n",
            "Epoch 7/20, Loss: 991077.0000\n",
            "Epoch 8/20, Loss: 990928.1875\n",
            "Epoch 9/20, Loss: 990676.0000\n",
            "Epoch 10/20, Loss: 990340.1250\n",
            "Epoch 11/20, Loss: 990015.9375\n",
            "Epoch 12/20, Loss: 989773.5000\n",
            "Epoch 13/20, Loss: 989578.5625\n",
            "Epoch 14/20, Loss: 989220.6250\n",
            "Epoch 15/20, Loss: 989109.4375\n",
            "Epoch 16/20, Loss: 988820.1875\n",
            "Epoch 17/20, Loss: 988622.4375\n",
            "Epoch 18/20, Loss: 988411.0000\n",
            "Epoch 19/20, Loss: 988197.3750\n",
            "Epoch 20/20, Loss: 987991.3125\n",
            "Average MPJPE: 1582.11 mm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reformer版 3D Pose Estimation\n",
        "## human3.6mでのコード\n",
        "## 実際にhuman3.6mデータセットを入れる必要あり\n",
        "## 入れるデータセットのパスをdata_path に代入\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from reformer_pytorch import Reformer, Autopadder\n",
        "import os\n",
        "import cv2\n",
        "from torchvision import transforms\n",
        "class Human36MDataset(Dataset):\n",
        "    def __init__(self, data_path, transform=None):\n",
        "        self.data_path = data_path\n",
        "        self.transform = transform\n",
        "        self.images, self.keypoints = self.load_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        images = []\n",
        "        keypoints = []\n",
        "        # データパスを探索し、画像とキーポイントデータを読み込む\n",
        "        for subject in os.listdir(self.data_path):\n",
        "            subject_path = os.path.join(self.data_path, subject)\n",
        "            if os.path.isdir(subject_path):\n",
        "                for sequence in os.listdir(subject_path):\n",
        "                    sequence_path = os.path.join(subject_path, sequence)\n",
        "                    if os.path.isdir(sequence_path):\n",
        "                        for file in os.listdir(sequence_path):\n",
        "                            if file.endswith('.jpg'):\n",
        "                                img_path = os.path.join(sequence_path, file)\n",
        "                                kp_path = img_path.replace('.jpg', '_keypoints.npy')\n",
        "                                if os.path.exists(kp_path):\n",
        "                                    img = cv2.imread(img_path)\n",
        "                                    kp = np.load(kp_path)\n",
        "                                    images.append(img)\n",
        "                                    keypoints.append(kp)\n",
        "        return images, keypoints\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        keypoints = self.keypoints[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, keypoints\n",
        "\n",
        "# Human3.6Mデータセットのパス\n",
        "# ここにHuman3.6Mデータセットを入れておく\n",
        "data_path = '/path/to/human36m'\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "dataset = Human36MDataset(data_path, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "def train_model(model, dataloader, criterion, optimizer, num_epochs=25):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader.dataset)\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}, Loss: {epoch_loss:.4f}')\n",
        "    return model\n",
        "def compute_mpjpe(predicted, true):\n",
        "    return torch.mean(torch.norm(predicted - true, dim=-1))  # 各キーポイント間の距離を計算し、平均を取る\n",
        "model = PoseReformer3D()\n",
        "criterion = nn.MSELoss()  # MPJPEを計算するための損失関数\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "model = train_model(model, dataloader, criterion, optimizer, num_epochs=25)\n",
        "model.eval()\n",
        "total_mpjpe = 0.0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in dataloader:\n",
        "        outputs = model(inputs)\n",
        "        mpjpe = compute_mpjpe(outputs, labels)\n",
        "        total_mpjpe += mpjpe.item() * inputs.size(0)\n",
        "\n",
        "average_mpjpe = total_mpjpe / len(dataloader.dataset)\n",
        "print(f'Average MPJPE: {average_mpjpe:.2f} mm')\n"
      ],
      "metadata": {
        "id": "6jwX6vvWCIS5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "b7837449-049a-44c4-947b-32ee43836380"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/path/to/human36m'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-2a41c60cecce>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.485\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.456\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.406\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.229\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.225\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m ])\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHuman36MDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-2a41c60cecce>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_path, transform)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeypoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-2a41c60cecce>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mkeypoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# データパスを探索し、画像とキーポイントデータを読み込む\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msubject\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0msubject_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/human36m'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZqWMvYrMFf3l"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMfbnTRuK+OcWrSI8NgLf94",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}